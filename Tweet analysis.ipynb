{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определения функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "conn = sqlite3.connect('M:\\\\sqlite3\\\\tweets.sqlite3')\n",
    "\n",
    "\n",
    "def dict_generator(indict, pre=None):\n",
    "    ''' Функция для обхода Nested structure внутри Json. Нужна только тогда когда структура \n",
    "            json-файла заранее неизвестна. для аргумента indict (строки json-файла) возвращает последовательность\n",
    "            списков, где последний элемент - значение (лист дерева),а все остальные - ключи вложенных словарей'''\n",
    "    pre = pre[:] if pre else []\n",
    "    if isinstance(indict, dict):\n",
    "        for key, value in indict.items():\n",
    "            if isinstance(value, dict):\n",
    "                for d in dict_generator(value, pre + [key]):\n",
    "                    yield d\n",
    "            elif isinstance(value, list) or isinstance(value, tuple):\n",
    "                for v in value:\n",
    "                    for d in dict_generator(v, pre +[key] ):\n",
    "                        yield d\n",
    "            else:\n",
    "                yield pre + [key, value]\n",
    "    else:\n",
    "        yield pre+[indict]\n",
    "        \n",
    "        \n",
    "def dict_to_df(dictname):\n",
    "    ''' Перевод словаря из Json в Data Frame и дублирование строк для создания денормализованной таблицы'''\n",
    "    def find_key_with_list(dictname):\n",
    "        for i in dictname:\n",
    "            if isinstance(dictname[i],list) or isinstance(dictname[i],tuple):\n",
    "                return i\n",
    "            \n",
    "    def make_lists_in_dict(dictname):\n",
    "        a=dictname.copy()\n",
    "        for j in a:\n",
    "            a[j]=[a[j]]\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    \n",
    "    def dict_to_table_structure(dictname,key_withlist):\n",
    "        #shalow copy вводного словаря, чтобы не изменять значение внутри функции\n",
    "        a=dictname.copy()\n",
    "     \n",
    "        if len(a[key_withlist])>1:\n",
    "\n",
    "            for j in [j for j in a if j!=key_withlist]:\n",
    "                a[j]=[a[j]]*len(a[key_withlist])\n",
    "            return a\n",
    "        elif len(a[key_withlist])==1:\n",
    "            a[key_withlist]=a[key_withlist][0]\n",
    "            a=make_lists_in_dict(a)\n",
    "            return a\n",
    "        elif len(a[key_withlist])==0:\n",
    "            a[key_withlist]=np.nan\n",
    "            a=make_lists_in_dict(a)\n",
    "            return a\n",
    "        \n",
    "\n",
    "    \n",
    "    return pd.DataFrame(dict_to_table_structure(dictname,find_key_with_list(dictname)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка в DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'name':[], 'text':[], 'country_code':[], 'display_url':[], 'lang':[], 'created_at':[], 'location':[]})\n",
    "filename=u'M:\\\\sqlite3\\\\three_minutes_tweets.json.txt'\n",
    "with open(filename,'r') as tweetFile:\n",
    "    for it,line in enumerate([line for line in tweetFile]):\n",
    "        #построчно обрабатываем файл с твитами\n",
    "        tweet=json.loads(line)\n",
    "        # создание целевого словаря, где на один твит приходится по одной записи для всех ключей, кроме 'display_url'\n",
    "        tweet_record=dict.fromkeys(['name', 'text', 'country_code', 'display_url', 'lang', 'created_at', 'location'])\n",
    "        tweet_record['display_url']=[]\n",
    "        \n",
    "        # отбрасываем записи с ключом delete\n",
    "        if not tweet.keys()==[u'delete']:\n",
    "            for a in dict_generator(tweet):\n",
    "            \n",
    "#                 if a is None:\n",
    "#                     pass\n",
    "            \n",
    "                if len(a)>=2:\n",
    "                    if a[-2] in tweet_record:\n",
    "\n",
    "                        if len(a)==3 and a[-2]==u'name':\n",
    "                            tweet_record['name']=a[-1]\n",
    "\n",
    "                        elif len(a)==2 and a[-2]==u'text':\n",
    "                            tweet_record['text']=a[-1]\n",
    "                        elif a[-2]==u'display_url':\n",
    "                            tweet_record[u'display_url'].append(a[-1])\n",
    "                            \n",
    "                        elif a[-2] in [u'country_code',u'lang',u'created_at',u'location']:\n",
    "                            tweet_record[a[-2]]=a[-1]\n",
    "                            \n",
    "        tweet_record[u'display_url']=list(set(tweet_record[u'display_url']))\n",
    "        \n",
    "        #Заменить на: df.to_sql(\"Tweet\", conn, if_exists=\"append\")\n",
    "        df=df.append(dict_to_df(tweet_record),ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тест для загруженного DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklines(thefile, whatlines):\n",
    "    return [x for i, x in enumerate(thefile) if i in whatlines]\n",
    "\n",
    "cnt=0\n",
    "for i in range(1000):\n",
    "    with open(filename,'r') as tweetFile:\n",
    "        line=picklines(tweetFile,[i])[0]\n",
    "\n",
    "\n",
    "        try:\n",
    "            line=json.loads(line)\n",
    "            if not all(df.loc[(df.name==line[u'user'][u'name']) & (df.text==line[u'text'])].created_at==line[u'created_at']):\n",
    "                print 'ERROR!!',i,line\n",
    "        except KeyError:\n",
    "            print i,line\n",
    "            cnt+=1\n",
    "        except TypeError:\n",
    "            print i,line\n",
    "            cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка в базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor()\n",
    "df=df.rename(columns={'text':'tweet_text'})\n",
    "\n",
    "df.to_sql(\"Tweet\", conn, if_exists=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для нормализации БД создаются отдельные таблицы для Твитов и для ссылок в Твитах (вторая нормальная форма)\n",
    "c.execute('''DROP TABLE Tweet_Table ; ''')\n",
    "c.execute('''DROP TABLE Url_Table ; ''')\n",
    "\n",
    "c.execute('''CREATE TABLE Tweet_Table\n",
    "                (\n",
    "                tweet_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "               created_at,name,\n",
    "               tweet_text,\n",
    "               country_code,\n",
    "               lang,\n",
    "               location,\n",
    "               tweet_sentiment\n",
    "               );\n",
    "                  ''')\n",
    "\n",
    "c.execute('''INSERT INTO Tweet_Table (created_at,name,tweet_text,country_code,lang,location,tweet_sentiment)\n",
    "                SELECT DISTINCT created_at,name,tweet_text,country_code,lang,location,tweet_sentiment FROM Tweet; ''')\n",
    "\n",
    "c.execute('''CREATE TABLE Url_Table\n",
    "                (\n",
    "                tweet_id,\n",
    "                display_url\n",
    "               );\n",
    "          ''')\n",
    "\n",
    "c.execute(''' INSERT INTO Url_Table (tweet_id,display_url)\n",
    "              SELECT t1.tweet_id,t2.display_url\n",
    "              FROM Tweet_Table t1\n",
    "              LEFT JOIN Tweet t2 ON t1.created_at=t2.created_at AND t1.name=t2.name AND t1.tweet_text=t2.tweet_text\n",
    "              WHERE t2.display_url<>\"None\"\n",
    "\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# подсчет настроений через поиск по словарю\n",
    "\n",
    "# tweets_for_analysis=df.loc[(df.lang=='en')].tweet_text\n",
    "\n",
    "\n",
    "\n",
    "dict_of_sentiment={}\n",
    "\n",
    "with open(u'M:\\\\sqlite3\\\\AFINN-111.txt') as f:\n",
    "    for line in f:\n",
    "        dict_of_sentiment[line.strip().split('\t')[0]]=int(line.strip().split('\t')[1])\n",
    "        \n",
    "\n",
    "        \n",
    "tweets_for_analysis=pd.read_sql('''SELECT tweet_id,tweet_text FROM Tweet_Table WHERE tweet_id IS NOT NULL;''',conn)\n",
    "\n",
    "\n",
    "# для каждого твита считается настроение и загружается в таблицу по одной записи\n",
    "\n",
    "for tweet_id,tweet in zip(tweets_for_analysis.tweet_id.values,tweets_for_analysis.tweet_text.values):\n",
    "    if not tweet is None: \n",
    "        # нижний регистр, очистка от пробелов и разделение по словам\n",
    "        bag_of_words=map(lambda x:x.lower(),tweet.strip().split(' '))\n",
    "        cnt=0\n",
    "        for i in bag_of_words:\n",
    "            if i in dict_of_sentiment:\n",
    "                cnt+=dict_of_sentiment[i]\n",
    "        # загружаем в БД только твиты с ненулевым настроением\n",
    "        if cnt!=0:\n",
    "            to_append=pd.DataFrame({'tweet_id':[tweet_id],'tweet_sentiment':[cnt]});\n",
    "            to_append.to_sql(\"Tweet_sentiment2\", conn, if_exists=\"append\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('''SELECT tweet_id,tweet_text FROM Tweet_Table''',conn,chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Найти страну-локацию-имя с максимальным и минимальным значением\n",
    "def most_group(field,x):\n",
    "    '''field - поле по которому данные группируются для подсчета минимального/максимального настроения \n",
    "        x - параметр принимающий значение 'MAX' или 'MIN'  '''\n",
    "        return pd.read_sql(''' \n",
    "\n",
    "                        SELECT '''+field+'''\n",
    "\n",
    "                        FROM\n",
    "                        (\n",
    "                        SELECT t2.'''+field+''',SUM(t1.tweet_sentiment) AS sentiment \n",
    "                        FROM Tweet_sentiment2 t1\n",
    "                        LEFT JOIN Tweet_Table t2 ON t1.tweet_id=t2.tweet_id\n",
    "                        GROUP BY '''+field+'''\n",
    "                        ) t1\n",
    "                        WHERE sentiment=(SELECT '''+x+ '''(sentiment) FROM \n",
    "                                                        (\n",
    "                                                        SELECT t2.'''+field+''',SUM(t1.tweet_sentiment) AS sentiment \n",
    "                                                        FROM Tweet_sentiment2 t1\n",
    "                                                        LEFT JOIN Tweet_Table t2 ON t1.tweet_id=t2.tweet_id\n",
    "                                                        WHERE '''+field+''' IS NOT NULL AND\n",
    "                                                        '''+field+'''<>''\n",
    "                                                        GROUP BY '''+field+'''\n",
    "                                                        ) t1\n",
    "                                        )\n",
    "\n",
    "\n",
    "\n",
    "                        ''',conn)[field].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Самая счастливая страна',most_group('country_code','MAX')\n",
    "print 'Самая несчастная страна',most_group('country_code','MIN')\n",
    "\n",
    "print 'Самая счастливая локация',most_group('location','MAX')\n",
    "print 'Самая несчастная локация',most_group('location','MIN')\n",
    "\n",
    "print 'Самый счастливый пользователь',most_group('name','MAX')\n",
    "print 'Самый несчастный пользователь',most_group('name','MIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(''' SELECT location,sentiment\n",
    "\n",
    "                        FROM\n",
    "                        (\n",
    "                        SELECT t2.location,SUM(t1.tweet_sentiment) AS sentiment \n",
    "                        FROM Tweet_sentiment2 t1\n",
    "                        LEFT JOIN Tweet_Table t2 ON t1.tweet_id=t2.tweet_id\n",
    "                        GROUP BY location ) t1\n",
    "                        ORDER BY sentiment DESC''',conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('''DROP TABLE Tweet_sentiment2;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
